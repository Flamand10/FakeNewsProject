{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Tokenize via NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#nltk stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#read csv\n",
    "df = pd.read_csv('news_sample.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size: 21102\n",
      "Vocabulary Size After Stopword Removal: 20756\n",
      "Reduction Rate After Stopword Removal: 1.64%\n",
      "Vocabulary Size After Stemming: 11827\n",
      "Reduction Rate After Stemming: 43.02%\n"
     ]
    }
   ],
   "source": [
    "#makes csv to a single string\n",
    "text = \" \".join(df.astype(str).agg(\" \".join, axis=1))\n",
    "\n",
    "#tokenize without punkt, only words\n",
    "word_tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "#original vocab length\n",
    "OG_vocab_size = len(set(word_tokens))\n",
    "\n",
    "#remove stopwords\n",
    "filtered_tokens = [w for w in word_tokens if w.lower() not in stop_words]\n",
    "filtered_vocab_size = len(set(filtered_tokens))\n",
    "\n",
    "#stopword reduction rate\n",
    "stopword_reduction_rate = ((OG_vocab_size - filtered_vocab_size) / OG_vocab_size * 100)\n",
    "\n",
    "#stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(w) for w in filtered_tokens]\n",
    "stemmed_vocab_size = len(set(stemmed_tokens))\n",
    "\n",
    "#stem reduction rate\n",
    "stemming_reduction_rate = ((filtered_vocab_size - stemmed_vocab_size) / filtered_vocab_size * 100)\n",
    "\n",
    "#print stuff\n",
    "print(f\"Original Vocabulary Size: {OG_vocab_size}\")\n",
    "print(f\"Vocabulary Size After Stopword Removal: {filtered_vocab_size}\")\n",
    "print(f\"Reduction Rate After Stopword Removal: {stopword_reduction_rate:.2f}%\")\n",
    "print(f\"Vocabulary Size After Stemming: {stemmed_vocab_size}\")\n",
    "print(f\"Reduction Rate After Stemming: {stemming_reduction_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we use the given nltk, we use pandas to load the csv file and we use regex for tokenize without punkt. It was very appropriate to use nltk to get a list of very commons stopwords in the text. We use pandas because it lead to a very simple way of loading the csv file. \n",
    "\n",
    "\n",
    "\n",
    "Vi er goated lets goo, tester ogs√•\n",
    "test igen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
