{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing columns: ['region', 'title', 'summary', 'link'] in bbc_articles.csv\n",
      "\n",
      "Stats for bbc_articles.csv:\n",
      "Original Vocabulary Size: 6732\n",
      "Vocabulary Size After Stopword Removal: 4849\n",
      "Reduction Rate After Stopword Removal: 27.97%\n",
      "Vocabulary Size After Stemming: 3707\n",
      "Reduction Rate After Stemming: 23.55%\n",
      "Successfully saved cleaned dataset: bbc_articles_cleaned.csv\n",
      "Processing columns: ['col_0', 'col_1', 'col_2', 'col_3', 'col_4', 'col_5', 'col_6', 'col_7', 'col_13'] in test.tsv\n",
      "\n",
      "Stats for test.tsv:\n",
      "Original Vocabulary Size: 7464\n",
      "Vocabulary Size After Stopword Removal: 4339\n",
      "Reduction Rate After Stopword Removal: 41.87%\n",
      "Vocabulary Size After Stemming: 3303\n",
      "Reduction Rate After Stemming: 23.88%\n",
      "Successfully saved cleaned dataset: test_cleaned.tsv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure necessary downloads\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def cleanF(file_path):\n",
    "    try:\n",
    "        #handle TSV files without proper headers\n",
    "        delimiter = ',' if file_path.endswith('.csv') else '\\t' #if it ends with .csv it assigns (comma) as the delimiter\n",
    "        header = None if file_path.endswith('.tsv') else 'infer'  #use no headers for TSV\n",
    "\n",
    "        #read file with adjusted headers\n",
    "        df = pd.read_csv(file_path, dtype=str, delimiter=delimiter, encoding='utf-8', \n",
    "                         engine='python', on_bad_lines='skip', header=header)\n",
    "\n",
    "        #if no headers used, use generic column names (col_0, col_1, ...)\n",
    "        if header is None:\n",
    "            df.columns = [f\"col_{i}\" for i in range(len(df.columns))]\n",
    "\n",
    "        #ensure stopwords are lowercase\n",
    "        stop_words = set(w.lower() for w in stopwords.words('english'))\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "        #vocab tracking\n",
    "        original_vocab = set()\n",
    "        filtered_vocab = set()\n",
    "        stemmed_vocab = set()\n",
    "\n",
    "        def clean_text(text):\n",
    "            if not isinstance(text, str):\n",
    "                return \"\"\n",
    "\n",
    "            #tokenization\n",
    "            word_tokens = word_tokenize(text)\n",
    "            original_vocab.update(word_tokens)\n",
    "\n",
    "            #stopword removal\n",
    "            filtered_tokens = [w.lower() for w in word_tokens if w.isalpha() and w.lower() not in stop_words]\n",
    "            filtered_vocab.update(filtered_tokens)\n",
    "\n",
    "            #stemming\n",
    "            stemmed_tokens = [stemmer.stem(w) for w in filtered_tokens]\n",
    "            stemmed_vocab.update(stemmed_tokens)\n",
    "\n",
    "            return \" \".join(stemmed_tokens)\n",
    "\n",
    "        #ignore numbers & JSON-like values, only text used\n",
    "        text_columns = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.contains(r'[a-zA-Z]').any()]\n",
    "        if not text_columns:\n",
    "            print(f\"No valid text columns detected in {file_path}. Skipping file.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Processing columns: {text_columns} in {file_path}\")\n",
    "\n",
    "        #apply cleaning function to detected text columns\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].apply(clean_text)\n",
    "\n",
    "        #print vocabulary statistics\n",
    "        OG_vocab_size = len(original_vocab)\n",
    "        filtered_vocab_size = len(filtered_vocab)\n",
    "        stemmed_vocab_size = len(stemmed_vocab)\n",
    "        stopword_reduction_rate = ((OG_vocab_size - filtered_vocab_size) / OG_vocab_size * 100) if OG_vocab_size else 0\n",
    "        stemming_reduction_rate = ((filtered_vocab_size - stemmed_vocab_size) / filtered_vocab_size * 100) if filtered_vocab_size else 0\n",
    "\n",
    "        print(f\"\\nStats for {file_path}:\")\n",
    "        print(f\"Original Vocabulary Size: {OG_vocab_size}\")\n",
    "        print(f\"Vocabulary Size After Stopword Removal: {filtered_vocab_size}\")\n",
    "        print(f\"Reduction Rate After Stopword Removal: {stopword_reduction_rate:.2f}%\")\n",
    "        print(f\"Vocabulary Size After Stemming: {stemmed_vocab_size}\")\n",
    "        print(f\"Reduction Rate After Stemming: {stemming_reduction_rate:.2f}%\")\n",
    "\n",
    "        #generate and save cleaned file\n",
    "        base_name, ext = os.path.splitext(file_path)\n",
    "        new_file_name = f\"{base_name}_cleaned{ext}\"\n",
    "        df.to_csv(new_file_name, index=False)\n",
    "        print(f\"Successfully saved cleaned dataset: {new_file_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "#files to process\n",
    "file_paths = ['bbc_articles.csv', 'test.tsv']\n",
    "\n",
    "#run processing function on each file\n",
    "for file_path in file_paths:\n",
    "    cleanF(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CaspersConda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
